# local-llama2-chat-documents
LLM llama2 running locally using streamlit uses your docs to provide you responses

1- download llama-2-7b-chat.ggmlv3.q2_K.bin from https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main

2- pip install -r requirements.txt

3- streamlit run app.py
